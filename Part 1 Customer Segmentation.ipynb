{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint\n",
    "import operator\n",
    "import time\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA    \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# !pip install mca\n",
    "# import mca\n",
    "import chardet\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "# azdias = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';', dtype=str)\n",
    "# customers = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';', dtype=str)\n",
    "azdias = pd.read_csv('Udacity_AZDIAS_052018.csv', sep=';', dtype=str)\n",
    "customers = pd.read_csv('Udacity_CUSTOMERS_052018.csv', sep=';', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be sure to add in a lot more cells (both markdown and code) to document your\n",
    "# approach and findings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias[\"LNR\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like each row has unique LNR value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[\"LNR\"].nunique() == customers.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.intersect1d(azdias[\"LNR\"], customers[\"LNR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe will help in replacing the values indicating missing data with `None` during the cleaning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_info = pd.read_csv(\"features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_cleaned = azdias.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in feat_info.iterrows():\n",
    "    attribute, information_level, var_type, missing, comment = row\n",
    "    if attribute in azdias_cleaned.columns:\n",
    "        values = missing.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "        replacement = {}\n",
    "        for value in values:\n",
    "            value = value.strip()\n",
    "            replacement[value] = None\n",
    "        azdias_cleaned.loc[:, attribute].replace(replacement, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an assessment of how much missing data there is in each column of the\n",
    "# dataset.\n",
    "missing = azdias_cleaned.isnull().sum()\n",
    "missing = missing[missing > 0]/(azdias.shape[0]) * 100\n",
    "missing.sort_values(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(missing, bins=20, facecolor='c', alpha=0.75)\n",
    "plt.xlabel('Percentage of missing value')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Distribution of missing value counts')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_20 = [col for col in azdias_cleaned.columns if (azdias[col].isnull().sum()/azdias_cleaned.shape[0]) * 100 > 20]\n",
    "print(missing_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to drop these columns during the data cleaning step as the columns with more than 20% missing values look like outliers among all the columns as most of the columns have less than 20% missing values.\n",
    "\n",
    "To decide on how to deal with the remaining columns with less than 20% missing values, I decided to divide the columns in two sets: one with less than or equal to x missing values and another with more than x missing values and observe the distribution of values for the columns in each set to see if there is any significant difference visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_less_than_20_missing = [col for col in azdias_cleaned.columns if col not in missing_20]\n",
    "missing = azdias_cleaned[cols_less_than_20_missing].isnull().sum()\n",
    "missing = missing[missing > 0]/(azdias_cleaned[cols_less_than_20_missing].shape[0]) * 100\n",
    "missing.sort_values(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(missing, bins=20, facecolor='c', alpha=0.75)\n",
    "plt.xlabel('Percentage of missing value')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Distribution of missing value counts')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_missing = azdias_cleaned[cols_less_than_20_missing][azdias_cleaned[cols_less_than_20_missing].isnull().sum(axis=1) < 20].reset_index(drop=True)\n",
    "high_missing = azdias_cleaned[cols_less_than_20_missing][azdias_cleaned[cols_less_than_20_missing].isnull().sum(axis=1) >= 20].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names_few = few_missing.columns\n",
    "\n",
    "fig, axes = plt.subplots(nrows=7, ncols=2, figsize=(20,30))\n",
    "sns.set(style=\"darkgrid\")\n",
    "for column in azdias_cleaned.columns[0:7]:\n",
    "    sns.countplot(few_missing.loc[:, column], ax=axes[n,0])\n",
    "    axes[n,0].set_title('Data with <= 20 values missing per row')\n",
    "    sns.countplot(high_missing.loc[:, column], ax=axes[n,1])\n",
    "    axes[n,1].set_title('Data with > 20 values missing per row')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above figures that there is significant difference between the distributions of the values in the rows with large number of missing values vs. low number of missing values. This suggests that the missing values are more likely to be present in rows with specific distribution of values in other columns with few missing values. This means that it is not a good idea to drop the rows with missing values and instead we need to fill them with some meaningful value. For the categorical, ordinal and interval type columns, it makes sense to fill the missing values with Mode of the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column \"flag indicating the former GDR/FRG\" contains categorical values 'W' and 'O' which needs to be converted into binary numerical values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of using in PCA, I have only retained columns that have less that 30 ordinal level values during the cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the method that encodes all the cleaning steps described above. This method can be called on both population data as well as customers data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_input):\n",
    "    \"\"\"\n",
    "    Takes data frame as input. \n",
    "    Performs required cleaning steps to convert it into a dataframe useful for further analysis.\n",
    "    Input:\n",
    "    - df_input : Dataframe to be cleaned\n",
    "    Output:\n",
    "    - df : Cleaned dataframe\n",
    "    \"\"\"\n",
    "    df = df_input.copy()\n",
    "    print (\"Copied\")\n",
    "    \n",
    "    for index, row in feat_info.iterrows():\n",
    "        attribute, information_level, var_type, missing, comment = row\n",
    "        if attribute in df.columns:\n",
    "            values = missing.replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "            replacement = {}\n",
    "            for value in values:\n",
    "                value = value.strip()\n",
    "                replacement[value] = None\n",
    "            df.loc[:, attribute].replace(replacement, inplace=True)\n",
    "    \n",
    "    print(\"Replaced unknown with None\")\n",
    "    df.replace({\"-1\": None, 'X': None, 'XX': None}, inplace=True)\n",
    "    print (\"Replaced -1, X, XX with None\")\n",
    "    recode = ['D19_BANKEN_DATUM', 'D19_BANKEN_OFFLINE_DATUM',\n",
    "       'D19_BANKEN_ONLINE_DATUM', 'D19_GESAMT_DATUM',\n",
    "       'D19_GESAMT_OFFLINE_DATUM', 'D19_GESAMT_ONLINE_DATUM',\n",
    "       'D19_TELKO_DATUM', 'D19_TELKO_OFFLINE_DATUM',\n",
    "       'D19_TELKO_ONLINE_DATUM', 'D19_VERSAND_DATUM',\n",
    "       'D19_VERSAND_OFFLINE_DATUM', 'D19_VERSAND_ONLINE_DATUM',\n",
    "       'D19_VERSI_DATUM', 'D19_VERSI_OFFLINE_DATUM',\n",
    "       'D19_VERSI_ONLINE_DATUM']\n",
    "    to_be_recoded = [col for col in df.columns if col in recode]\n",
    "    df[to_be_recoded] = df[to_be_recoded].replace(\"10\", \"0\")\n",
    "    col_select = (df.isna().sum(axis=0)/df.shape[0]) <= 0.20    \n",
    "    df = df.loc[:, col_select]\n",
    "    print (\"Retained columns with only few missing values\")\n",
    "    df = df.loc[:, df.nunique() <= 30]\n",
    "    print (\"Retained columns with limited levels\")\n",
    "    def fill_mode(col):\n",
    "        return col.fillna(col.mode()[0])\n",
    "    df = df.apply(fill_mode, axis=0)\n",
    "    print (\"Filled Nan with Mode\")\n",
    "    \n",
    "    numbers = [str(x) for x in range(100)]\n",
    "    for col in df.columns:\n",
    "        level = 0\n",
    "        for value in df[col].unique():\n",
    "            if value not in numbers:\n",
    "                df.loc[df[col] == value, col] = level\n",
    "                print(col + \" \" + str(df[col].unique()))                \n",
    "                print(\"Replaced {} with {}\".format(value, level))\n",
    "                level += 1\n",
    "    df = df.astype(float)\n",
    "    print (\"Converted to numeric\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = np.intersect1d(customers.columns, azdias.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust = clean_data(customers[common_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = clean_data(azdias[common_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust.to_csv(\"cust.csv\", index=False, header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.to_csv(\"pop.csv\", index=False, header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust = pd.read_csv(\"cust.csv\", sep=\";\")\n",
    "pop = pd.read_csv(\"pop.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the dataframes population and customers need to have same columns to be able to apply Principal Component Analysis technique with PCA model fitted using population data and then used to transform the customer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols = np.intersect1d(pop.columns, cust.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_float = pop[common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_scaled = pd.DataFrame(scaler.fit_transform(pop_float.values), columns=pop_float.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_float.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_float = cust[common_cols].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_scaled = pd.DataFrame(scaler.transform(cust_float.values), columns=cust_float.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_pca = pca.fit_transform(pop_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCA, I decided to retain number components that account for 80% for variance in the dataset. Below graph visualizes that it retained to 40 components which could explain 80% of variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = min(np.where(np.cumsum(pca.explained_variance_ratio_)>0.8)[0]+1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1],True)\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(pca.explained_variance_ratio_, label='Variance',)\n",
    "ax2.plot(np.cumsum(pca.explained_variance_ratio_), label='Cumulative Variance',color = 'orange');\n",
    "ax.set_title('n_components needed for >%80 explained variance: {}'.format(n_components));\n",
    "ax.axvline(n_components, linestyle='dashed', color='black')\n",
    "ax2.axhline(np.cumsum(pca.explained_variance_ratio_)[n_components], linestyle='dashed', color='black')\n",
    "fig.legend(loc=(0.6,0.2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_pca = pca.fit_transform(pop_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmeans_score(data, k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    model = kmeans.fit(data)\n",
    "    score = np.abs(model.score(data))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "ks = list(range(2,31, 5))\n",
    "for k in ks:\n",
    "    scores.append(get_kmeans_score(pop_pca, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks, scores, marker='o');\n",
    "plt.xlabel('K');\n",
    "plt.ylabel('SSE');\n",
    "plt.title('SSE vs. K');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the reduction in Standard Squared Error of becomes very slow after 20 clusters, I decided to divide the population data into 20 clusters and proceed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=k, random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pop = kmeans.fit(pop_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_pred = model_pop.predict(pop_pca) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_pca = pca.transform(cust_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_pred = model_pop.predict(cust_pca) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_prop = []\n",
    "customers_prop = []\n",
    "x = range(1, k+1)\n",
    "for i in range(1, k+1):\n",
    "    general_prop.append((pop_pred == i).sum()/len(pop_pred))\n",
    "    customers_prop.append((cust_pred == i).sum()/len(cust_pred))\n",
    "\n",
    "\n",
    "df_prop = pd.DataFrame({'cluster' : x, 'prop_general' : general_prop, 'prop_customers':customers_prop})\n",
    "\n",
    "#ax = sns.countplot(x='index', y = df_general['prop_1', 'prop_2'], data=df_general )\n",
    "df_prop.plot(x='cluster', y = ['prop_general', 'prop_customers'], kind='bar', figsize=(9,6))\n",
    "plt.ylabel('proportion of people in each cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, it's visible that the customers are highly represented in the cluster number 3 and 6. Also some of the customers are represented in clusters 1, 5 and 16. Now let's try to interpret the components of the cluster which is mostly represents the consumers to understand the characteristics of the people that are assigned to that cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[\"diff\"] = (df_prop[\"prop_customers\"] - df_prop[\"prop_general\"])/df_prop[\"prop_general\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop[\"diff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_diff = df_prop.sort_values(by=[\"diff\"], ascending=False)[\"cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out the most highly representative cluster for the customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_cust_cluster = sorted_by_diff.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_likely_cust_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing out all the clusters that the customers are represented by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(cust_pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_in_most_likely_cust_cluster = cust_pca[cust_pred == most_likely_cust_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_in_most_likely_cust_cluster[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_in_least_likely_cust_cluster = cust_pca[cust_pred == least_likely_cust_cluster, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_weights(pca, i):\n",
    "    weight_map = {}\n",
    "    for counter, feature in enumerate(common_cols):\n",
    "        weight_map[feature] = pca.components_[i][counter]\n",
    "    \n",
    "    sorted_weights = sorted(weight_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return sorted_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to find out the principal component in which the customers in highly representative cluster differ significantly from the rest of the population. For that I am relying upon the distance of mean of the values of each component in terms of standard deviations and sorting them by that distance in decreasing order to see which component has highest distance from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_significance = pd.DataFrame({\"comp\": range(pop_pca.shape[1]), \"dist_in_std\" : (cust_in_most_likely_cust_cluster.mean(axis=0) - pop_pca.mean(axis=0))/pop_pca.std(axis=0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_significance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_significance[\"abs_dist_in_std\"] = comp_significance[\"dist_in_std\"].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sorted = comp_significance.sort_values(by=[\"abs_dist_in_std\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that component number 25 is having highest distance of values from the rest of the population. We can take a look at the weights associated with the original input variables for that component to see which variables have highest positive or negative influence on deciding the value of this component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(pca_weights(pca, comp_sorted.iloc[0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the original attributes that have highest absolute weight for the component for e.g. D19_KONSUMTYP_MAX (people who are morel likely to modern, informed or inactive in purchasing behavior), D19_VERSICHERUNGEN (people who are less likely to buy insurance), D19_BUCH_CD (people who are less likely to buy book or CD), D19_GESAMT_ANZ_12 (people who have high transactional activities in last 12 months), D19_BANKEN_DIREKT (people who are more likely to bank directly), D19_GESAMT_OFFLINE_DATUM (people who are more likely to have performed offline transactions recently), D19_HAUS_DEKO (people who are more likely to buy items related to house decoration), D19_DROGERIEARTIKEL (people who are more likely to purchase from drugstores). D19_WEIN_FEINKOST (people who are likely to buy wine). \n",
    "\n",
    "It feels that these traits can be observed in older people, which suggests that the most representative cluster of the population for large majority of customer base of this company is that containing relatively older people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have completed Part 2 and 3 in seperate notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
